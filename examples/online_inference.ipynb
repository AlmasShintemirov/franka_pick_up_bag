{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rclpy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from subscriber import Subscriber, get_observation\n",
    "# from publisher import Publisher\n",
    "from sensor_msgs.msg import Image\n",
    "from std_msgs.msg import String, Float32MultiArray\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rclpy.init(args=None)\n",
    "# node = Subscriber()\n",
    "# msg = wait_for_message(node, Image, '/Camera_wrist_rgb')\n",
    "# wrist_image_array = np.array(msg.data).reshape((512, 512, 3))\n",
    "# plt.imshow(wrist_image_array)\n",
    "# msg = wait_for_message(node, String, '/language_topic')\n",
    "# print(msg.data)\n",
    "# rclpy.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irobotics/anaconda3/envs/octo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import cv2\n",
    "import jax\n",
    "import tensorflow_datasets as tfds\n",
    "import tqdm\n",
    "import mediapy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_CHECKPOINTS = \"/media/irobotics/Transcend/finetuned_checkpoints/v4_checkpoints/\"\n",
    "PATH_DATASET_ROSBAG = \"/media/irobotics/Transcend/isaacsim_data/v4_test/\"\n",
    "PATH_DATASET_TFDS = '/media/irobotics/Transcend/tensorflow_datasets/v4_test/example_dataset/1.0.0/'\n",
    "PATH_INFERENCE_RESULTS = \"/media/irobotics/Transcend/inference_result/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-20 14:44:23.618324: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-20 14:44:23.618411: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-20 14:44:23.642676: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-20 14:44:24.366424: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "WARNING:jax._src.xla_bridge:CUDA backend failed to initialize: Found cuDNN version 0, but JAX was built against version 8600, which is newer. The copy of cuDNN that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "WARNING:root:Using old attention computation from released December models.\n",
      "/home/irobotics/anaconda3/envs/octo/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from octo.model.octo_model import OctoModel\n",
    "\n",
    "model = OctoModel.load_pretrained(PATH_CHECKPOINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-20 14:45:09.188512: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# create RLDS dataset builder\n",
    "builder = tfds.builder_from_directory(builder_dir=PATH_DATASET_TFDS)\n",
    "ds = builder.as_dataset(split='train[:2]')\n",
    "iterator = iter(ds)\n",
    "episode = next(iterator)\n",
    "# sample episode + resize to 256x256 (default third-person cam resolution)\n",
    "steps = list(episode['steps'])\n",
    "images = [cv2.resize(np.array(step['observation']['image']), (256, 256)) for step in steps]\n",
    "# extract goal image & language instruction\n",
    "goal_image = images[-1]\n",
    "language_instruction = steps[100]['language_instruction'].numpy().decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'tasks' contains extra items compared to example_batch: {'pad_mask_dict/image_primary', 'image_primary'}\n"
     ]
    }
   ],
   "source": [
    "# create `task` dict\n",
    "# task = model.create_tasks(goals={\"image_primary\": goal_image[None]})   # for goal-conditioned\n",
    "task = model.create_tasks(goals={\"image_primary\": goal_image})\n",
    "task = model.create_tasks(texts=[language_instruction])                  # for language conditioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rclpy.init(args=None)\n",
    "# node = Subscriber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Array([[ 2.66343802e-01,  3.74182872e-02,  4.15885121e-01,\n",
      "         1.57695543e-02],\n",
      "       [ 2.73925245e-01,  5.00609241e-02,  4.17830229e-01,\n",
      "         8.35826620e-04],\n",
      "       [ 2.74249911e-01,  5.01887053e-02,  4.08265203e-01,\n",
      "         1.64688807e-02],\n",
      "       [ 2.67579526e-01,  6.72994927e-02,  4.11384076e-01,\n",
      "         8.01343843e-03],\n",
      "       [ 2.69764274e-01,  6.84235618e-02,  4.05125558e-01,\n",
      "         3.53298360e-03],\n",
      "       [ 2.64368266e-01,  8.29432160e-02,  4.00553286e-01,\n",
      "        -2.05529039e-03],\n",
      "       [ 2.64549553e-01,  8.60974714e-02,  4.02104467e-01,\n",
      "         1.90375447e-02],\n",
      "       [ 2.63056129e-01,  9.15554464e-02,  4.00507987e-01,\n",
      "         3.10508651e-03],\n",
      "       [ 2.59429336e-01,  9.75181982e-02,  3.88898432e-01,\n",
      "         3.68680269e-03],\n",
      "       [ 2.59059846e-01,  1.02926433e-01,  3.91461790e-01,\n",
      "         1.02729574e-02],\n",
      "       [ 2.61255771e-01,  1.10972784e-01,  3.81284833e-01,\n",
      "        -2.93942401e-03],\n",
      "       [ 2.56184518e-01,  1.35343164e-01,  3.74965280e-01,\n",
      "         7.41024921e-03],\n",
      "       [ 2.49366581e-01,  1.43234268e-01,  3.69098037e-01,\n",
      "        -1.55766611e-03],\n",
      "       [ 2.49426022e-01,  1.44323751e-01,  3.74236107e-01,\n",
      "         7.59073673e-03],\n",
      "       [ 2.46192411e-01,  1.49556026e-01,  3.71914536e-01,\n",
      "        -3.19357868e-03],\n",
      "       [ 2.43569151e-01,  1.60575017e-01,  3.61815721e-01,\n",
      "        -4.93616390e-04],\n",
      "       [ 2.39545301e-01,  1.70464754e-01,  3.57394099e-01,\n",
      "         6.17155666e-03],\n",
      "       [ 2.40104839e-01,  1.72715247e-01,  3.48605216e-01,\n",
      "         8.10126215e-03],\n",
      "       [ 2.35978886e-01,  1.73783898e-01,  3.49077791e-01,\n",
      "         6.47212937e-03],\n",
      "       [ 2.28330761e-01,  1.92057565e-01,  3.39204043e-01,\n",
      "         1.59315486e-02],\n",
      "       [ 2.20608205e-01,  2.06557140e-01,  3.33689958e-01,\n",
      "        -1.11474644e-03],\n",
      "       [ 2.21747786e-01,  2.09640965e-01,  3.34324241e-01,\n",
      "         2.01894604e-02],\n",
      "       [ 2.19495803e-01,  2.08731860e-01,  3.23877901e-01,\n",
      "        -4.41445224e-03],\n",
      "       [ 2.15055153e-01,  2.13355318e-01,  3.26977760e-01,\n",
      "         1.82318746e-03],\n",
      "       [ 2.15349823e-01,  2.16043189e-01,  3.23177725e-01,\n",
      "        -3.95619823e-03],\n",
      "       [ 2.07303390e-01,  2.25022197e-01,  3.22280079e-01,\n",
      "        -3.10020405e-04],\n",
      "       [ 2.04372466e-01,  2.33207226e-01,  3.13310266e-01,\n",
      "         4.04736213e-03],\n",
      "       [ 2.03322709e-01,  2.46619210e-01,  3.11384827e-01,\n",
      "        -6.94068894e-03],\n",
      "       [ 2.02230945e-01,  2.50606656e-01,  3.07343960e-01,\n",
      "         3.55102797e-03],\n",
      "       [ 1.99672729e-01,  2.60183305e-01,  3.03014815e-01,\n",
      "         5.79148950e-03],\n",
      "       [ 1.98794663e-01,  2.61185974e-01,  2.98861831e-01,\n",
      "        -1.51782120e-02],\n",
      "       [ 2.00699314e-01,  2.73233235e-01,  2.93560058e-01,\n",
      "         3.67618725e-03],\n",
      "       [ 1.95657879e-01,  2.68442631e-01,  2.97403097e-01,\n",
      "         2.14009057e-03],\n",
      "       [ 2.01038569e-01,  2.75802970e-01,  2.82646835e-01,\n",
      "        -5.61046693e-03],\n",
      "       [ 1.97655186e-01,  2.74120897e-01,  2.93353885e-01,\n",
      "         3.32333404e-03],\n",
      "       [ 1.96672872e-01,  2.88547248e-01,  2.84089833e-01,\n",
      "        -4.25794860e-03],\n",
      "       [ 1.97314516e-01,  3.06614399e-01,  2.78109312e-01,\n",
      "         3.63756157e-03],\n",
      "       [ 2.00564489e-01,  2.99920678e-01,  2.78991848e-01,\n",
      "         1.34298140e-02],\n",
      "       [ 2.01362431e-01,  3.04701835e-01,  2.71424025e-01,\n",
      "         1.49018895e-02],\n",
      "       [ 2.02682883e-01,  3.06084901e-01,  2.80797690e-01,\n",
      "         9.95732751e-03],\n",
      "       [ 2.03233823e-01,  3.28453839e-01,  2.78363049e-01,\n",
      "         1.59712997e-03],\n",
      "       [ 2.03911111e-01,  3.22387069e-01,  2.68885612e-01,\n",
      "        -4.11377434e-04],\n",
      "       [ 2.13369265e-01,  3.31254214e-01,  2.67504781e-01,\n",
      "         8.33126809e-03],\n",
      "       [ 2.10221171e-01,  3.36154312e-01,  2.64506131e-01,\n",
      "         1.55376960e-02],\n",
      "       [ 2.11293727e-01,  3.36599290e-01,  2.70334959e-01,\n",
      "         9.24605317e-03],\n",
      "       [ 2.10446492e-01,  3.40950698e-01,  2.66168863e-01,\n",
      "         4.48503561e-04],\n",
      "       [ 2.15373054e-01,  3.46799821e-01,  2.62572378e-01,\n",
      "         5.68738766e-03],\n",
      "       [ 2.22436056e-01,  3.40422392e-01,  2.59105623e-01,\n",
      "         1.18431775e-02],\n",
      "       [ 2.26512492e-01,  3.55845034e-01,  2.68964142e-01,\n",
      "         8.69710930e-03],\n",
      "       [ 2.24367172e-01,  3.55935276e-01,  2.55061924e-01,\n",
      "        -1.98723355e-04]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# # run inference loop, this model only uses 3rd person image observations for bridge\n",
    "# # collect predicted and true actions\n",
    "# pred_actions= []\n",
    "# # true_actions = steps['action']\n",
    "# input_images = get_observation(node, Image, '/Camera_rgb', (256, 256, 3))\n",
    "# input_images_wrist = get_observation(node, Image, '/Camera_wrist_rgb', (512, 512, 3))\n",
    "\n",
    "# observation = {\n",
    "#     'image_primary': input_images,\n",
    "#     'image_wrist': input_images_wrist,\n",
    "#     'timestep_pad_mask': np.full((1, input_images.shape[1]), True, dtype=bool)\n",
    "# }\n",
    "\n",
    "# # one step actions\n",
    "# actions = model.sample_actions(\n",
    "#     observation, \n",
    "#     task, \n",
    "#     unnormalization_statistics=model.dataset_statistics[\"action\"], \n",
    "#     rng=jax.random.PRNGKey(0)\n",
    "# )\n",
    "# actions = actions[0] # remove batch dim\n",
    "\n",
    "# pred_actions.append(actions)\n",
    "# print(pred_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### publish calculated actions\n",
    "# publisher = node.create_publisher(Float32MultiArray, 'online_eff_topic', 10)\n",
    "# pub_msg = Float32MultiArray()\n",
    "# # pub_msg.data = [0.2663438022136688, 0.03741828724741936, 0.4158851206302643, 0.015769554302096367]\n",
    "# pub_msg.data = actions[0,:].tolist()\n",
    "# while rclpy.ok():\n",
    "#     publisher.publish(pub_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "### publish random actions\n",
    "# publisher = node.create_publisher(Float32MultiArray, 'online_eff_topic', 10)\n",
    "# pub_msg = Float32MultiArray()\n",
    "# pub_msg.data = [0.5, 0.5, 0.5, 0.01]\n",
    "# # publisher.publish(pub_msg)\n",
    "# # while rclpy.ok():\n",
    "# #     publisher.publish(pub_msg)\n",
    "# for i in range(50):\n",
    "#     publisher.publish(pub_msg)\n",
    "#     sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node.destroy_node()\n",
    "# rclpy.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### while loop: publisher included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rclpy.init(args=None)\n",
    "node = Subscriber()\n",
    "publisher = node.create_publisher(Float32MultiArray, 'online_eff_topic', 10)\n",
    "pub_msg = Float32MultiArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'observations' is missing items compared to example_batch: {'timestep', 'pad_mask_dict/timestep', 'pad_mask_dict/image_wrist', 'pad_mask_dict/proprio', 'task_completed', 'pad_mask_dict/image_primary', 'proprio'}\n",
      "WARNING:root:No pad_mask_dict found. Nothing will be masked.\n",
      "WARNING:root:No pad_mask_dict found. Nothing will be masked.\n",
      "WARNING:root:No observation inputs matching ('proprio',) were found.Skipping tokenizer entirely.\n",
      "WARNING:root:Skipping observation tokenizer: obs_proprio\n",
      "WARNING:root:Using old attention computation from released December models.\n"
     ]
    }
   ],
   "source": [
    "# run inference loop, this model only uses 3rd person image observations for bridge\n",
    "# collect predicted and true actions\n",
    "pred_actions= []\n",
    "# true_actions = steps[:]['action']\n",
    "while True:\n",
    "\n",
    "    input_images = get_observation(node, Image, '/Camera_rgb', (256, 256, 3))\n",
    "    input_images_wrist = get_observation(node, Image, '/Camera_wrist_rgb', (512, 512, 3))\n",
    "\n",
    "    observation = {\n",
    "        'image_primary': input_images,\n",
    "        'image_wrist': input_images_wrist,\n",
    "        'timestep_pad_mask': np.full((1, input_images.shape[1]), True, dtype=bool)\n",
    "    }\n",
    "\n",
    "    # one step actions\n",
    "    actions = model.sample_actions(\n",
    "        observation, \n",
    "        task, \n",
    "        unnormalization_statistics=model.dataset_statistics[\"action\"], \n",
    "        rng=jax.random.PRNGKey(0)\n",
    "    )\n",
    "    actions = actions[0] # remove batch dim\n",
    "\n",
    "    # pred_actions.append(actions)\n",
    "    \n",
    "    # publish actions to robot\n",
    "    pub_msg.data = actions[0,:].tolist()\n",
    "\n",
    "    for i in range(50):\n",
    "        publisher.publish(pub_msg)\n",
    "        sleep(0.1)\n",
    "\n",
    "    # TODO: check if action execution is done\n",
    "\n",
    "    # TODO: how to check if episode is done, then break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node.destroy_node()\n",
    "# rclpy.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(PATH_INFERENCE_RESULTS + \"online_inference_one_step_action.npy\", actions[0,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "octo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
